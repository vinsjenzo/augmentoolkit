# 
API:
  # Use your locally-loaded model for both "large" (Q-A gen + polish)
  # and "small" (bulk chunk work) steps:
  LARGE_MODEL: deepseek-llm-7b-chat
  LARGE_BASE_URL: http://localhost:80/v1/
  LARGE_API_KEY: local
  LARGE_MODE: api

  SMALL_MODEL: deepseek-llm-7b-chat
  SMALL_BASE_URL: http://localhost:80/v1/
  SMALL_API_KEY: local
  SMALL_MODE: api

HUGGINGFACE:
  HUB_PATH: ""
  PRIVATE: false
  PUSH_TO_HUB: false

PATH:
  INPUT: ./input/scraped_text           # your cleaned .txt/.md docs go here
  OUTPUT: ./output
  PROMPTS: ./prompts
  DEFAULT_PROMPTS: ./prompts

PHASE:
  PHASE_INDEX: 3          # run the full 4-phase pipeline in one go
  WORK_IN_PHASES: false

SKIP:
  ANSWER_RELEVANCY_CHECK: false
  QUESTION_CHECK: false
  FILTER_CHUNKS: false
  REPAIR_QA_TUPLES: false
  CONVERSATION_GENERATION: false

SYSTEM:
  CHUNK_SIZE: 1900        # adjust if you need longer context
  COMPLETION_MODE: false  # set to true only if you really want legacy /completions
  CONCURRENCY_LIMIT: 8    # start lowâ€”bump after you confirm LM Studio can handle it
  CONVERSATION_INSTRUCTIONS: |
    For this conversation, generate a helpful dialogue between a domain-expert
    AI assistant and a human end-user.
  DOUBLE_CHECK_COUNTER: 1
  DO_NOT_USE_SYSTEM_PROMPTS: false

  FINAL_ASSISTANT_PROMPTS_NO_RAG: [
  'You are a helpful AI assistant.',
  'You are A VASTLY intelligent ARTIFICIAL INTELLIGENCE with DOMAIN-EXPERT KNOWLEDGE from a variety of fields.\nUSE your knowledge to be helpful and truthfully answer questions about the world.',
  "u are ai asstant plz answr questions"] # a wide variety of system prompts helps the AI learn better. What, you expect your users to spell things right?
  FINAL_ASSISTANT_PROMPTS_RAG: [
  'You are a helpful AI assistant. Some knowledge:\n\n{data}',
  '\n{data}\n\nYou are an AI domain expert. Answer questions',
  'You are an AI with vast knowledge. Here is some potentially-relevant context:\n\n{data}\n\nAnswer questions according to your knowledge.']

  RAG_FAILURE_PERCENTAGE: 0.1
  STOP: true              # stop after all phases complete
  USE_SUBSET: false       # set true+SUBSET_SIZE for testing on a small slice
  SUBSET_SIZE: 20
  USE_FILENAMES: false

SCRAPING:
  USE_GUTENBERG: false    # you've already scraped your own webpages
  START_URL: ""
  MAX_BOOKS: 0
  MAX_FAILURES: 0
